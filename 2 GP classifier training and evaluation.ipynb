{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF,WhiteKernel,RationalQuadratic\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import expit\n",
    "from scipy.linalg import cholesky, cho_solve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import signal\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the classification model used in lecture\n",
    "class mnist_nn_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        # just need to define it the other way, and then can just return the outputs from any given layer in a similar way to classify\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,3,1)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.drop1 = nn.Dropout2d(.25)\n",
    "        self.flat = nn.Flatten(1)\n",
    "        self.lin1 = nn.Linear(9216,128)\n",
    "        self.drop2 = nn.Dropout2d(.25)\n",
    "        self.lin2 = nn.Linear(128,16)\n",
    "        self.lin3 = nn.Linear(16,10)\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        # log likelihood for a batch of data\n",
    "        return - nn.functional.cross_entropy(self.f(x), y, reduction='none')\n",
    "    \n",
    "    def last_layer(self, x):\n",
    "        # computing in parts so it is easy to pull out different internal pieces\n",
    "        pooled = self.pool(nn.functional.relu(self.conv2(nn.functional.relu(self.conv1(x)))))\n",
    "        lin1 = self.lin1(self.flat(self.drop1(nn.functional.relu(pooled))))\n",
    "        lin2 = self.lin2(self.drop2(nn.functional.relu(lin1)))\n",
    "        return lin2\n",
    "    \n",
    "    def f(self, x):\n",
    "        return self.lin3(nn.functional.relu(self.last_layer(x)))\n",
    "    def classify(self,x):\n",
    "        # class probabilities for a single data point\n",
    "        q = self.f(torch.as_tensor(x)[None,...])[0]\n",
    "        return nn.functional.softmax(q, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = mnist_nn_model()\n",
    "optimizer = optim.Adam(nn_model.parameters())\n",
    "nn_model.train(mode = False)\n",
    "nn_model.load_state_dict(torch.load('cnn_classifier.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the datasets\n",
    "X_orig = pd.read_csv('X_orig_binary.csv')\n",
    "Y_orig = pd.read_csv('Y_orig_binary.csv')\n",
    "X_test = pd.read_csv('X_test_binary.csv')\n",
    "Y_test = pd.read_csv('Y_test_binary.csv')\n",
    "X_white_noise = pd.read_csv('X_white_noise_binary.csv')\n",
    "Y_white_noise = pd.read_csv('Y_white_noise_binary.csv')\n",
    "X_motion_blur = pd.read_csv('X_motion_blur_binary.csv')\n",
    "Y_motion_blur = pd.read_csv('Y_motion_blur_binary.csv')\n",
    "X_reduced_contrast = pd.read_csv('X_reduced_contrast_binary.csv')\n",
    "Y_reduced_contrast = pd.read_csv('Y_reduced_contrast_binary.csv')\n",
    "X_fgm10 = pd.read_csv('X_fgm_10_binary.csv')\n",
    "Y_fgm10 = pd.read_csv('Y_fgm_10_binary.csv')\n",
    "X_fgm25 = pd.read_csv('X_fgm_25_binary.csv')\n",
    "Y_fgm25 = pd.read_csv('Y_fgm_25_binary.csv')\n",
    "X_fgm50 = pd.read_csv('X_fgm_50_binary.csv')\n",
    "Y_fgm50 = pd.read_csv('Y_fgm_50_binary.csv')\n",
    "X_fgm100 = pd.read_csv('X_fgm_100_binary.csv')\n",
    "Y_fgm100 = pd.read_csv('Y_fgm_100_binary.csv')\n",
    "X_pgd10 = pd.read_csv('X_pgd_10_binary.csv')\n",
    "Y_pgd10 = pd.read_csv('Y_pgd_10_binary.csv')\n",
    "X_pgd25 = pd.read_csv('X_pgd_25_binary.csv')\n",
    "Y_pgd25 = pd.read_csv('Y_pgd_25_binary.csv')\n",
    "X_pgd50 = pd.read_csv('X_pgd_50_binary.csv')\n",
    "Y_pgd50 = pd.read_csv('Y_pgd_50_binary.csv')\n",
    "X_pgd100 = pd.read_csv('X_pgd_100_binary.csv')\n",
    "Y_pgd100 = pd.read_csv('Y_pgd_100_binary.csv')\n",
    "\n",
    "del X_orig['Unnamed: 0']\n",
    "del Y_orig['Unnamed: 0']\n",
    "del X_test['Unnamed: 0']\n",
    "del Y_test['Unnamed: 0']\n",
    "del X_white_noise['Unnamed: 0']\n",
    "del Y_white_noise['Unnamed: 0']\n",
    "del X_motion_blur['Unnamed: 0']\n",
    "del Y_motion_blur['Unnamed: 0']\n",
    "del X_reduced_contrast['Unnamed: 0']\n",
    "del Y_reduced_contrast['Unnamed: 0']\n",
    "del X_fgm10['Unnamed: 0']\n",
    "del Y_fgm10['Unnamed: 0']\n",
    "del X_fgm25['Unnamed: 0']\n",
    "del Y_fgm25['Unnamed: 0']\n",
    "del X_fgm50['Unnamed: 0']\n",
    "del Y_fgm50['Unnamed: 0']\n",
    "del X_fgm100['Unnamed: 0']\n",
    "del Y_fgm100['Unnamed: 0']\n",
    "del X_pgd10['Unnamed: 0']\n",
    "del Y_pgd10['Unnamed: 0']\n",
    "del X_pgd25['Unnamed: 0']\n",
    "del Y_pgd25['Unnamed: 0']\n",
    "del X_pgd50['Unnamed: 0']\n",
    "del Y_pgd50['Unnamed: 0']\n",
    "del X_pgd100['Unnamed: 0']\n",
    "del Y_pgd100['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframes to np arrays for easier processing\n",
    "X_orig = X_orig.to_numpy()\n",
    "Y_orig = Y_orig.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "X_white_noise = X_white_noise.to_numpy()\n",
    "Y_white_noise = Y_white_noise.to_numpy()\n",
    "X_motion_blur = X_motion_blur.to_numpy()\n",
    "Y_motion_blur = Y_motion_blur.to_numpy()\n",
    "X_reduced_contrast = X_reduced_contrast.to_numpy()\n",
    "Y_reduced_contrast = Y_reduced_contrast.to_numpy()\n",
    "X_fgm10 = X_fgm10.to_numpy()\n",
    "Y_fgm10 = Y_fgm10.to_numpy()\n",
    "X_fgm25 = X_fgm25.to_numpy()\n",
    "Y_fgm25 = Y_fgm25.to_numpy()\n",
    "X_fgm50 = X_fgm50.to_numpy()\n",
    "Y_fgm50 = Y_fgm50.to_numpy()\n",
    "X_fgm100 = X_fgm100.to_numpy()\n",
    "Y_fgm100 = Y_fgm100.to_numpy()\n",
    "X_pgd10 = X_pgd10.to_numpy()\n",
    "Y_pgd10 = Y_pgd10.to_numpy()\n",
    "X_pgd25 = X_pgd25.to_numpy()\n",
    "Y_pgd25 = Y_pgd25.to_numpy()\n",
    "X_pgd50 = X_pgd50.to_numpy()\n",
    "Y_pgd50 = Y_pgd50.to_numpy()\n",
    "X_pgd100 = X_pgd100.to_numpy()\n",
    "Y_pgd100 = Y_pgd100.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the full images for the neural network classifier as well\n",
    "# load the standard mnist dataset\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root = 'pytorch-data/',  # where to put the files\n",
    "    download = True,         # if files aren't here, download them\n",
    "    train = True,            # whether to import the test or the train subset\n",
    "    # PyTorch uses PyTorch tensors internally, not numpy arrays, so convert them.\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    ")\n",
    "mnist_batched = torch.utils.data.DataLoader(mnist_train, batch_size=100)\n",
    "X_orig_img = torch.stack([img for img,lbl in mnist_train if lbl==0 or lbl ==1])\n",
    "Y_orig_img = [lbl for img,lbl in mnist_train if lbl == 0 or lbl == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root = 'pytorch-data/',  # where to put the files\n",
    "    download = True,         # if files aren't here, download them\n",
    "    train = False,            # whether to import the test or the train subset\n",
    "    # PyTorch uses PyTorch tensors internally, not numpy arrays, so convert them.\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    ")\n",
    "X_test_img = torch.stack([img for img,lbl in mnist_train if lbl==0 or lbl ==1])\n",
    "Y_test_img = [lbl for img,lbl in mnist_train if lbl == 0 or lbl == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, io\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/nmnist/mnist-with-awgn.mat\")\n",
    "\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "wn_train_x = data['train_x'].reshape(60000,1,28,28)\n",
    "wn_train_y = data['train_y']\n",
    "decoded = np.argmax(wn_train_y, axis=1)\n",
    "wn_y_bin = np.array([lbl for lbl in decoded if lbl == 0 or lbl == 1])\n",
    "wn_x_bin = torch.stack([torch.Tensor(img)/255 for i,img in enumerate(wn_train_x) if decoded[i]==0 or decoded[i]==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/nmnist/mnist-with-motion-blur.mat\")\n",
    "\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    mb_train_x = data['train_x'].reshape(60000,1,28,28)\n",
    "    mb_train_y = data['train_y']\n",
    "    decoded = np.argmax(mb_train_y, axis=1)\n",
    "    mb_y_bin = np.array([lbl for lbl in decoded if lbl == 0 or lbl == 1])\n",
    "    mb_x_bin = torch.stack([torch.Tensor(img)/255 for i,img in enumerate(mb_train_x) if decoded[i]==0 or decoded[i]==1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/nmnist/mnist-with-reduced-contrast-and-awgn.mat\")\n",
    "\n",
    "with io.BytesIO(r.content) as f:\n",
    "    data = scipy.io.loadmat(f)\n",
    "    rc_train_x = data['train_x'].reshape(60000,1,28,28)\n",
    "    rc_train_y = data['train_y']\n",
    "    decoded = np.argmax(rc_train_y, axis=1)\n",
    "    rc_y_bin = np.array([lbl for lbl in decoded if lbl == 0 or lbl == 1])\n",
    "    rc_x_bin = torch.stack([torch.Tensor(img)/255 for i,img in enumerate(rc_train_x) if decoded[i]==0 or decoded[i]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fgm_10_images = torch.load('X_fgm_10_images.pt')\n",
    "X_fgm_25_images = torch.load('X_fgm_25_images.pt')\n",
    "X_fgm_50_images = torch.load('X_fgm_50_images.pt')\n",
    "X_fgm_100_images = torch.load('X_fgm_100_images.pt')\n",
    "X_pgd_10_images = torch.load('X_pgd_10_images.pt')\n",
    "X_pgd_25_images = torch.load('X_pgd_25_images.pt')\n",
    "X_pgd_50_images = torch.load('X_pgd_50_images.pt')\n",
    "X_pgd_100_images = torch.load('X_pgd_100_images.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gaussian Process classifier with a basic RBF kernel added\n",
    "# to a white noise kernel\n",
    "kernel =  RBF(1) + WhiteKernel(noise_level=.1)\n",
    "gpc = GaussianProcessClassifier(kernel = kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can fit a Gaussian Process on the original MNIST dataset. With that classifier trained, we will be able to compare the measurements of epistemic uncertainty and classifier accuracy across different perturbations of the dataset. \n",
    "\n",
    "Fitting a Gaussian Process takes $O(n^3)$ time in the number of datapoints trained on, so I am using only a subset of the datapoints available here to make processing time reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessClassifier(kernel=RBF(length_scale=1) + WhiteKernel(noise_level=0.1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.fit(X_orig[:3000], Y_orig[:3000].reshape(3000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gpc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3613/1636410498.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we can also save this model for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpc_trained.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gpc' is not defined"
     ]
    }
   ],
   "source": [
    "# we can also save this model for future use\n",
    "import pickle\n",
    "pickle.dump(gpc, open('gpc_trained.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "gpc = pickle.load(open('gpc_trained.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained GP classifier, we will first compare the accuracy of this model with the accuracy of the neural net we trained in notebook 1 on the original MNIST dataset and the noisy datasets. This serves as a sanity check that our classifier works, and a benchmark for the relative performance we can expect.\n",
    "\n",
    "First, we define some helper functions to return accuracy scores for both the GP classifier and the original NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_gpc(gpc,x,y):\n",
    "    pred = gpc.predict(x)\n",
    "    compare = (pred == y)[0]\n",
    "    return sum(1 for i in compare if i) / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_nn(nn,x,y):\n",
    "    correct = 0\n",
    "    for i,img in enumerate(x):\n",
    "        probs = nn.classify(img)\n",
    "        pred = np.argmax(probs.detach().numpy())\n",
    "        if pred == y[i]:\n",
    "            correct += 1\n",
    "    return correct / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST train :3000 dataset, the GP classifier has an accuracy of 0.9996666666666667 and the NN classifier has an accuracy of 0.997\n"
     ]
    }
   ],
   "source": [
    "print(f\"MNIST train :3000 dataset, the GP classifier has an accuracy of {score_gpc(gpc,X_orig[:3000],Y_orig[:3000].reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_orig_img[:3000], Y_orig_img[:3000])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST train :3000 dataset, the GP classifier has an accuracy of 1.0 and the NN classifier has an accuracy of 0.9973098810139679\n"
     ]
    }
   ],
   "source": [
    "print(f\"MNIST train 3000: dataset, the GP classifier has an accuracy of {score_gpc(gpc,X_orig[3000:],Y_orig[3000:].reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_orig_img[3000:], Y_orig_img[3000:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST test dataset, the GP classifier has an accuracy of 1.0 and the NN classifier has an accuracy of 0.9972364784840111\n"
     ]
    }
   ],
   "source": [
    "print(f\"MNIST test dataset, the GP classifier has an accuracy of {score_gpc(gpc,X_test,Y_test.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_test_img, Y_test_img)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the dataset with added white noise, the GP classifier has an accuracy of 0.9924200552704303 and the NN classifier has an accuracy of 0.7394393999210422\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the dataset with added white noise, the GP classifier has an accuracy of {score_gpc(gpc,X_white_noise,Y_white_noise.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, wn_x_bin, wn_y_bin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the dataset with motion blur, the GP classifier has an accuracy of 0.9973154362416108 and the NN classifier has an accuracy of 0.9900513225424398\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the dataset with motion blur, the GP classifier has an accuracy of {score_gpc(gpc,X_motion_blur,Y_motion_blur.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, mb_x_bin, mb_y_bin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the dataset with reduced contrast and added white noise, the GP classifier has an accuracy of 0.6577181208053692 and the NN classifier has an accuracy of 0.36628503750493485\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the dataset with reduced contrast and added white noise, the GP classifier has an accuracy of {score_gpc(gpc,X_reduced_contrast,Y_reduced_contrast.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, rc_x_bin, rc_y_bin)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the fgm 10 data, the GP classifier has an accuracy of 0.9971575207264114 and the NN classifier has an accuracy of 0.88251085669167\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the fgm 10 data, the GP classifier has an accuracy of {score_gpc(gpc,X_fgm10,Y_fgm10.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_fgm_10_images, Y_fgm10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the fgm 25 data, the GP classifier has an accuracy of 0.8656 and the NN classifier has an accuracy of 0.1986\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the fgm 25 data, the GP classifier has an accuracy of {score_gpc(gpc,X_fgm25,Y_fgm25.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_fgm_25_images, Y_fgm25)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the fgm 50 data, the GP classifier has an accuracy of 0.7002 and the NN classifier has an accuracy of 0.053\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the fgm 50 data, the GP classifier has an accuracy of {score_gpc(gpc,X_fgm50,Y_fgm50.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_fgm_50_images, Y_fgm50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the fgm 100 data, the GP classifier has an accuracy of 0.5576 and the NN classifier has an accuracy of 0.0316\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the fgm 100 data, the GP classifier has an accuracy of {score_gpc(gpc,X_fgm100,Y_fgm100.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_fgm_100_images, Y_fgm100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the pgd 10 data, the GP classifier has an accuracy of 0.9933333333333333 and the NN classifier has an accuracy of 0.09933333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the pgd 10 data, the GP classifier has an accuracy of {score_gpc(gpc,X_pgd10,Y_pgd10.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_pgd_10_images, Y_pgd10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the pgd 25 data, the GP classifier has an accuracy of 0.7233333333333334 and the NN classifier has an accuracy of 0.004\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the pgd 25 data, the GP classifier has an accuracy of {score_gpc(gpc,X_pgd25,Y_pgd25.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_pgd_25_images, Y_pgd25)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the pgd 50 data, the GP classifier has an accuracy of 0.49666666666666665 and the NN classifier has an accuracy of 0.0006666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the pgd 50 data, the GP classifier has an accuracy of {score_gpc(gpc,X_pgd50,Y_pgd50.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_pgd_50_images, Y_pgd50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the pgd 100 data, the GP classifier has an accuracy of 0.4646666666666667 and the NN classifier has an accuracy of 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"On the pgd 100 data, the GP classifier has an accuracy of {score_gpc(gpc,X_pgd100,Y_pgd100.reshape(1,-1))} and the NN classifier has an accuracy of {score_nn(nn_model, X_pgd_100_images, Y_pgd100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these initial results, we can see that the GP classifier performance is satisfactory. However, it is not fair to say that the NN performance is unsatisfactory because the NN is trained to classify all 10 digits, while the GP only chooses between 2. Since the focus of this project is on the evaluation of epistemic uncertainty in a GP classifier, this discrepancy is unimportant, and the tests above serve only as a sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can begin to explore the uncertainty in this classifier on different datasets. First, I will just look at the average uncertainty over all the datasets to understand how well the classifier 'thinks' it knows the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_orig_first = []\n",
    "u_orig_second = []\n",
    "u_test = []\n",
    "u_white_noise = []\n",
    "u_motion_blur = []\n",
    "u_reduced_contrast = []\n",
    "u_fgm10 = []\n",
    "u_fgm25 = []\n",
    "u_fgm50 = []\n",
    "u_fgm100 = []\n",
    "u_pgd10 = []\n",
    "u_pgd25 = []\n",
    "u_pgd50 = []\n",
    "u_pgd100 = []\n",
    "gpc.predict_proba(X_orig[:3000], uncertainty = u_orig_first)\n",
    "gpc.predict_proba(X_orig[3000:], uncertainty = u_orig_second)\n",
    "gpc.predict_proba(X_test, uncertainty = u_test)\n",
    "gpc.predict_proba(X_white_noise, uncertainty = u_white_noise)\n",
    "gpc.predict_proba(X_motion_blur, uncertainty = u_motion_blur)\n",
    "gpc.predict_proba(X_reduced_contrast, uncertainty = u_reduced_contrast)\n",
    "gpc.predict_proba(X_fgm10, uncertainty = u_fgm10)\n",
    "gpc.predict_proba(X_fgm25, uncertainty = u_fgm25)\n",
    "gpc.predict_proba(X_fgm50, uncertainty = u_fgm50)\n",
    "gpc.predict_proba(X_fgm100, uncertainty = u_fgm100)\n",
    "gpc.predict_proba(X_pgd10, uncertainty = u_pgd10)\n",
    "gpc.predict_proba(X_pgd25, uncertainty = u_pgd25)\n",
    "gpc.predict_proba(X_pgd50, uncertainty = u_pgd50)\n",
    "gpc.predict_proba(X_pgd100, uncertainty = u_pgd100)\n",
    "u_orig_first = np.array(u_orig_first)\n",
    "u_orig_second = np.array(u_orig_second)\n",
    "u_test = np.array(u_test)\n",
    "u_white_noise = np.array(u_white_noise)\n",
    "u_motion_blur = np.array(u_motion_blur)\n",
    "u_reduced_contrast = np.array(u_reduced_contrast)\n",
    "u_fgm10 = np.array(u_fgm10)\n",
    "u_fgm25 = np.array(u_fgm25)\n",
    "u_fgm50 = np.array(u_fgm50)\n",
    "u_fgm100 = np.array(u_fgm100)\n",
    "u_pgd10 = np.array(u_pgd10)\n",
    "u_pgd25 = np.array(u_pgd25)\n",
    "u_pgd50 = np.array(u_pgd50)\n",
    "u_pgd100 = np.array(u_pgd100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_total = np.concatenate([u_orig_first, u_orig_second], axis=1)\n",
    "u_total.shape\n",
    "plt.hist(u_total[0])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Uncertainty Score\")\n",
    "plt.savefig(\"uncertainty_trainset.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg uncertainty original dataset: 0.26461952675577394\n",
      "Avg uncertainty original dataset: 0.265736011602283\n",
      "Avg uncertainty original dataset: 0.26122360566986935\n",
      "Avg uncertainty white noise dataset: 0.33641428580158533\n",
      "Avg uncertainty motion blur dataset: 0.2371890128783516\n",
      "Avg uncertainty reduced contrast dataset: 0.3920097316459086\n",
      "Avg uncertainty FGM, eps = .1 dataset: 0.33038952362142887\n",
      "Avg uncertainty FGM, eps = .25 dataset: 0.4663663257186983\n",
      "Avg uncertainty FGM, eps = .5 dataset: 0.5786015014606509\n",
      "Avg uncertainty FGM, eps = 1 dataset: 0.8496510669234898\n",
      "Avg uncertainty PGD, eps = .1 dataset: 0.6013098050427522\n",
      "Avg uncertainty PGD, eps = .25 dataset: 0.9859089753844558\n",
      "Avg uncertainty PGD, eps = .5 dataset: 0.9997566555601982\n",
      "Avg uncertainty PGD, eps = 1 dataset: 1.0000070607326048\n"
     ]
    }
   ],
   "source": [
    "print(f\"Avg uncertainty original dataset: {u_orig_first.mean()}\")\n",
    "print(f\"Avg uncertainty original dataset: {u_orig_second.mean()}\")\n",
    "print(f\"Avg uncertainty original dataset: {u_test.mean()}\")\n",
    "print(f\"Avg uncertainty white noise dataset: {u_white_noise.mean()}\")\n",
    "print(f\"Avg uncertainty motion blur dataset: {u_motion_blur.mean()}\")\n",
    "print(f\"Avg uncertainty reduced contrast dataset: {u_reduced_contrast.mean()}\")\n",
    "print(f\"Avg uncertainty FGM, eps = .1 dataset: {u_fgm10.mean()}\")\n",
    "print(f\"Avg uncertainty FGM, eps = .25 dataset: {u_fgm25.mean()}\")\n",
    "print(f\"Avg uncertainty FGM, eps = .5 dataset: {u_fgm50.mean()}\")\n",
    "print(f\"Avg uncertainty FGM, eps = 1 dataset: {u_fgm100.mean()}\")\n",
    "print(f\"Avg uncertainty PGD, eps = .1 dataset: {u_pgd10.mean()}\")\n",
    "print(f\"Avg uncertainty PGD, eps = .25 dataset: {u_pgd25.mean()}\")\n",
    "print(f\"Avg uncertainty PGD, eps = .5 dataset: {u_pgd50.mean()}\")\n",
    "print(f\"Avg uncertainty PGD, eps = 1 dataset: {u_pgd100.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just this simple analysis, we can notice several things:\n",
    "* the epistemic uncertainty for the original dataset is very small even though the classifier was only trained on a subset of these images\n",
    "* As the value of epsilon (maximum perturbation from original) increases for the adversarial examples, the uncertainty increases as well.\n",
    "* If we compare the uncertainty results with the accuracy results from above, we can see that higher uncertainty and lower accuracy are not always correlated.\n",
    "\n",
    "We will keep these in mind in notebook 3 when we explore evaluation metrics for classification models with a measure of epistemic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, look at log likelihood of each different dataset\n",
    "# this repeats much of the Laplace estimation used in fitting the model as well\n",
    "def avg_log_likelihood_dataset(gpc, x, y, max_iter=100):\n",
    "    # marginal log likelihood, won't actually use this\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_adj = label_encoder.fit_transform(y)\n",
    "    f = np.zeros_like(y_adj, dtype=np.float64)\n",
    "    K = gpc.kernel(x)\n",
    "    log_marginal_likelihood = -np.inf\n",
    "    for _ in range(max_iter):\n",
    "        # Line 4\n",
    "        pi = expit(f)\n",
    "        W = pi * (1 - pi)\n",
    "        # Line 5\n",
    "        W_sr = np.sqrt(W)\n",
    "        W_sr_K = W_sr[:, np.newaxis] * K\n",
    "        B = np.eye(W.shape[0]) + W_sr_K * W_sr\n",
    "        L = cholesky(B, lower=True)\n",
    "        # Line 6\n",
    "        b = W * f + (y_adj - pi)\n",
    "        # Line 7\n",
    "        a = b - W_sr * cho_solve((L, True), W_sr_K.dot(b))\n",
    "        # Line 8\n",
    "        f = K.dot(a)\n",
    "\n",
    "        # Line 10: Compute log marginal likelihood in loop and use as\n",
    "        #          convergence criterion\n",
    "        lml = (\n",
    "            -0.5 * a.T.dot(f)\n",
    "            - np.log1p(np.exp(-(y_adj * 2 - 1) * f)).sum()\n",
    "            - np.log(np.diag(L)).sum()\n",
    "        )\n",
    "        # Check if we have converged (log marginal likelihood does\n",
    "        # not decrease)\n",
    "        if lml - log_marginal_likelihood < 1e-10:\n",
    "            break\n",
    "        log_marginal_likelihood = lml\n",
    "    return log_marginal_likelihood / y.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_log_likelihood_dataset(gpc, x, y):\n",
    "    probs = gpc.predict_proba(x)\n",
    "    total_loglik = 0\n",
    "    for i,true in enumerate(y):\n",
    "        total_loglik += np.log(probs[i][true])[0]\n",
    "    return total_loglik / y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.008248755210182007"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_likelihood_dataset(gpc, X_orig[3000:], Y_orig[3000:].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00834267163133084"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_likelihood_dataset(gpc, X_test, Y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood of white noise dataset: -0.13347116623966235\n",
      "Likelihood of motion blur dataset: -0.034519726576218435\n",
      "Likelihood of reduced contrast dataset: -0.4958644138933707\n",
      "Likelihood of fgm10 dataset: -0.05851114503239912\n",
      "Likelihood of fgm25 dataset: -0.33906669496168745\n",
      "Likelihood of fgm50 dataset: -0.5782588023658218\n",
      "Likelihood of fgm100 dataset: -0.6862984052090476\n",
      "Likelihood of pgd10 dataset: -0.16596928785654042\n",
      "Likelihood of pgd25 dataset: -0.6305748550111833\n",
      "Likelihood of pgd50 dataset: -0.6937118179825639\n",
      "Likelihood of pgd100 dataset: -0.6933279128781854\n"
     ]
    }
   ],
   "source": [
    "print (f\"Likelihood of white noise dataset: {avg_log_likelihood_dataset(gpc, X_white_noise, Y_white_noise.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of motion blur dataset: {avg_log_likelihood_dataset(gpc, X_motion_blur, Y_motion_blur.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of reduced contrast dataset: {avg_log_likelihood_dataset(gpc, X_reduced_contrast, Y_reduced_contrast.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of fgm10 dataset: {avg_log_likelihood_dataset(gpc, X_fgm10, Y_fgm10.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of fgm25 dataset: {avg_log_likelihood_dataset(gpc, X_fgm25, Y_fgm25.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of fgm50 dataset: {avg_log_likelihood_dataset(gpc, X_fgm50, Y_fgm50.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of fgm100 dataset: {avg_log_likelihood_dataset(gpc, X_fgm100, Y_fgm100.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of pgd10 dataset: {avg_log_likelihood_dataset(gpc, X_pgd10, Y_pgd10.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of pgd25 dataset: {avg_log_likelihood_dataset(gpc, X_pgd25, Y_pgd25.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of pgd50 dataset: {avg_log_likelihood_dataset(gpc, X_pgd50, Y_pgd50.reshape(-1,1))}\")\n",
    "print (f\"Likelihood of pgd100 dataset: {avg_log_likelihood_dataset(gpc, X_pgd100, Y_pgd100.reshape(-1,1))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel2 = 5 * RBF(1) + WhiteKernel(noise_level=0.1)\n",
    "kernel3 = RationalQuadratic(length_scale=1.0, alpha=1.5) + WhiteKernel(noise_level=0.1)\n",
    "gpc2 = GaussianProcessClassifier(kernel = kernel2)\n",
    "gpc3 = GaussianProcessClassifier(kernel = kernel3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__alpha is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianProcessClassifier(kernel=RationalQuadratic(alpha=1.5, length_scale=1) + WhiteKernel(noise_level=0.1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc2.fit(X_orig[:3000], Y_orig[:3000].reshape(3000,))\n",
    "gpc3.fit(X_orig[:3000], Y_orig[:3000].reshape(3000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gpc2, open('gpc2_trained.pkl','wb'))\n",
    "pickle.dump(gpc3, open('gpc3_trained.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999210422424003"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_gpc(gpc,X_orig,Y_orig.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would be good to do a little more of a demonstration of the log likelihood stuff\n",
    "# and what happens with the other kernels. Is there a better way to look at kernels \n",
    "# than just trying a few different ones? I could find a couple bad examples\n",
    "# to make things clear I guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uncertainty_combined = np.append(u_orig_first, u_orig_second)\n",
    "max_likely = train_uncertainty_combined.mean() + 1.96 * np.sqrt(train_uncertainty_combined.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate % likely out of distribution\n",
    "def likely_ood(max_likely, uncertainties):\n",
    "    return sum(1 for x in uncertainties if x > max_likely) / uncertainties.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likely_ood(max_likely, u_orig_first[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likely ood original first: 0.046\n",
      "likely ood original second: 0.048525607863424726\n",
      "likely ood test: 0.052955082742316785\n",
      "likely ood white noise dataset: 0.03434662455586261\n",
      "likely ood motion blur dataset: 0.0043426766679826295\n",
      "likely ood reduced contrast dataset: 0.06971969996052112\n",
      "likely ood FGM, eps = .1 dataset: 0.17268061587050929\n",
      "likely ood FGM, eps = .25 dataset: 0.4328\n",
      "likely ood FGM, eps = .5 dataset: 0.7792\n",
      "likely ood FGM, eps = 1 dataset: 0.985\n",
      "likely ood PGD, eps = .1 dataset: 0.8526666666666667\n",
      "likely ood PGD, eps = .25 dataset: 1.0\n",
      "likely ood PGD, eps = .5 dataset: 1.0\n",
      "likely ood PGD, eps = 1 dataset: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"likely ood original first: {likely_ood(max_likely, u_orig_first[0])}\")\n",
    "print(f\"likely ood original second: {likely_ood(max_likely, u_orig_second[0])}\")\n",
    "print(f\"likely ood test: {likely_ood(max_likely, u_test[0])}\")\n",
    "print(f\"likely ood white noise dataset: {likely_ood(max_likely, u_white_noise[0])}\")\n",
    "print(f\"likely ood motion blur dataset: {likely_ood(max_likely, u_motion_blur[0])}\")\n",
    "print(f\"likely ood reduced contrast dataset: {likely_ood(max_likely, u_reduced_contrast[0])}\")\n",
    "print(f\"likely ood FGM, eps = .1 dataset: {likely_ood(max_likely, u_fgm10[0])}\")\n",
    "print(f\"likely ood FGM, eps = .25 dataset: {likely_ood(max_likely, u_fgm25[0])}\")\n",
    "print(f\"likely ood FGM, eps = .5 dataset: {likely_ood(max_likely, u_fgm50[0])}\")\n",
    "print(f\"likely ood FGM, eps = 1 dataset: {likely_ood(max_likely, u_fgm100[0])}\")\n",
    "print(f\"likely ood PGD, eps = .1 dataset: {likely_ood(max_likely, u_pgd10[0])}\")\n",
    "print(f\"likely ood PGD, eps = .25 dataset: {likely_ood(max_likely, u_pgd25[0])}\")\n",
    "print(f\"likely ood PGD, eps = .5 dataset: {likely_ood(max_likely, u_pgd50[0])}\")\n",
    "print(f\"likely ood PGD, eps = 1 dataset: {likely_ood(max_likely, u_pgd100[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12665, 12665)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.kernel(X_orig).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1500)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpc.kernel(X_pgd10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
